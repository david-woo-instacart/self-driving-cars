{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KITTI Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /Users/davidwoo/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied: tqdm==4.11.2 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from moviepy)\n",
      "Requirement already satisfied: imageio==2.1.2 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from moviepy)\n",
      "Requirement already satisfied: decorator==4.0.11 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from moviepy)\n",
      "Requirement already satisfied: numpy in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from moviepy)\n",
      "Requirement already satisfied: pillow in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from imageio==2.1.2->moviepy)\n",
      "Requirement already satisfied: olefile in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from pillow->imageio==2.1.2->moviepy)\n",
      "Requirement already satisfied: numpy in /Users/davidwoo/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied: pykitti in /Users/davidwoo/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied: matplotlib in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from pykitti)\n",
      "Requirement already satisfied: numpy in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from pykitti)\n",
      "Requirement already satisfied: six>=1.10 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: python-dateutil in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: functools32 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: subprocess32 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: pytz in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from matplotlib->pykitti)\n",
      "Requirement already satisfied: opencv-python in /Users/davidwoo/anaconda/lib/python2.7/site-packages\n",
      "Requirement already satisfied: numpy>=1.11.1 in /Users/davidwoo/anaconda/lib/python2.7/site-packages (from opencv-python)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy\n",
    "!pip install numpy\n",
    "!pip install pykitti\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prerequisites\n",
    "\n",
    "#!pip3 install pykitti\n",
    "#!pip3 install moviepy\n",
    "\n",
    "# download the synced and rectified dataset. do not need the unsynced dataset\n",
    "#wget http://kitti.is.tue.mpg.de/kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip\n",
    "\n",
    "#wget http://kitti.is.tue.mpg.de/kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_tracklets.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unzip files\n",
    "#import zipfile\n",
    "#targetdir = \"KITTI_data\"\n",
    "#with zipfile.ZipFile(\"/home/ubuntu/2011_09_26_drive_0001_sync.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(targetdir)\n",
    "#    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(\"/home/ubuntu/2011_09_26_calib.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(targetdir)\n",
    "#    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with zipfile.ZipFile(\"/home/ubuntu/2011_09_26_drive_0001_tracklets.zip\",\"r\") as zip_ref:\n",
    "#    zip_ref.extractall(targetdir)\n",
    "#    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pykitti\n",
    "\n",
    "# Change this to the directory where you store KITTI data\n",
    "basedir = \"/Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset\"\n",
    "\n",
    "def load_dataset(date, drive, calibrated=False, frame_range=None):\n",
    "    \"\"\"\n",
    "    Loads the dataset with `date` and `drive`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    date        : Dataset creation date.\n",
    "    drive       : Dataset drive.\n",
    "    calibrated  : Flag indicating if we need to parse calibration data. Defaults to `False`.\n",
    "    frame_range : Range of frames. Defaults to `None`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Loaded dataset of type `raw`.\n",
    "    \"\"\"\n",
    "    dataset = pykitti.raw(basedir, date, drive)\n",
    "\n",
    "    # Load the data\n",
    "    if calibrated:\n",
    "        dataset._load_calib()  # Calibration data are accessible as named tuples\n",
    "\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "    print('\\nDrive: ' + str(dataset.drive))\n",
    "    print('\\nFrame range: ' + str(dataset.frames))\n",
    "\n",
    "    if calibrated:\n",
    "        print('\\nIMU-to-Velodyne transformation:\\n' + str(dataset.calib.T_velo_imu))\n",
    "        print('\\nGray stereo pair baseline [m]: ' + str(dataset.calib.b_gray))\n",
    "        print('\\nRGB stereo pair baseline [m]: ' + str(dataset.calib.b_rgb))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from source import parseTrackletXML as xmlParser\n",
    "\n",
    "def load_tracklets_for_frames(n_frames, xml_path):\n",
    "    \"\"\"\n",
    "    Loads dataset labels also referred to as tracklets, saving them individually for each frame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_frames    : Number of frames in the dataset.\n",
    "    xml_path    : Path to the tracklets XML.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of dictionaries with integer keys corresponding to absolute frame numbers and arrays as values. First array\n",
    "    contains coordinates of bounding box vertices for each object in the frame, and the second array contains objects\n",
    "    types as strings.\n",
    "    \"\"\"\n",
    "    #print(xml_path)\n",
    "    tracklets = xmlParser.parseXML(xml_path)\n",
    "\n",
    "    frame_tracklets = {}\n",
    "    frame_tracklets_types = {}\n",
    "    for i in range(n_frames):\n",
    "        frame_tracklets[i] = []\n",
    "        frame_tracklets_types[i] = []\n",
    "\n",
    "    # loop over tracklets\n",
    "    for i, tracklet in enumerate(tracklets):\n",
    "        # this part is inspired by kitti object development kit matlab code: computeBox3D\n",
    "        h, w, l = tracklet.size\n",
    "        # in velodyne coordinates around zero point and without orientation yet\n",
    "        trackletBox = np.array([\n",
    "            [-l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2],\n",
    "            [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2],\n",
    "            [0.0, 0.0, 0.0, 0.0, h, h, h, h]\n",
    "        ])\n",
    "        \n",
    "        print(tracklets)\n",
    "        # loop over all data in tracklet\n",
    "        for translation, rotation, state, occlusion, truncation, amtOcclusion, amtBorders, absoluteFrameNumber in tracklet:\n",
    "            # determine if object is in the image; otherwise continue\n",
    "            if truncation not in (xmlParser.TRUNC_IN_IMAGE, xmlParser.TRUNC_TRUNCATED):\n",
    "                continue\n",
    "            # re-create 3D bounding box in velodyne coordinate system\n",
    "            yaw = rotation[2]  # other rotations are supposedly 0\n",
    "            assert np.abs(rotation[:2]).sum() == 0, 'object rotations other than yaw given!'\n",
    "            rotMat = np.array([\n",
    "                [np.cos(yaw), -np.sin(yaw), 0.0],\n",
    "                [np.sin(yaw), np.cos(yaw), 0.0],\n",
    "                [0.0, 0.0, 1.0]\n",
    "            ])\n",
    "            cornerPosInVelo = np.dot(rotMat, trackletBox) + np.tile(translation, (8, 1)).T\n",
    "            frame_tracklets[absoluteFrameNumber] = frame_tracklets[absoluteFrameNumber] + [cornerPosInVelo]\n",
    "            frame_tracklets_types[absoluteFrameNumber] = frame_tracklets_types[absoluteFrameNumber] + [\n",
    "                tracklet.objectType]\n",
    "\n",
    "    return (frame_tracklets, frame_tracklets_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset downloaded from [KITTI website](http://www.cvlibs.net/datasets/kitti/raw_data.php). \n",
    "\n",
    "[2011_09_26_drive_0001 (0.4 GB)](http://kitti.is.tue.mpg.de/kitti/raw_data/2011_09_26_drive_0001/2011_09_26_drive_0001_sync.zip)\n",
    "\n",
    "* **Length**: 114 frames (00:11 minutes)\n",
    "* **Image resolution**: `1392 x 512` pixels\n",
    "* **Labels**: 12 Cars, 0 Vans, 0 Trucks, 0 Pedestrians, 0 Sitters, 2 Cyclists, 1 Trams, 0 Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method raw._load_calib of <pykitti.raw.raw object at 0x110252860>>\n"
     ]
    }
   ],
   "source": [
    "basedir = \"/Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset\"\n",
    "date = '2011_09_26'\n",
    "drive = '0048'\n",
    "dataset = pykitti.raw(basedir, date, drive)\n",
    "\n",
    "print(dataset._load_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Drive: 2011_09_26_drive_0048_sync\n",
      "\n",
      "Frame range: None\n",
      "\n",
      "IMU-to-Velodyne transformation:\n",
      "[[ 1.      0.0008 -0.002  -0.8087]\n",
      " [-0.0008  0.9999 -0.0148  0.3196]\n",
      " [ 0.002   0.0148  0.9999 -0.7997]\n",
      " [ 0.      0.      0.      1.    ]]\n",
      "\n",
      "Gray stereo pair baseline [m]: 0.537150600501\n",
      "\n",
      "RGB stereo pair baseline [m]: 0.532725440079\n",
      "Parsing tracklet file /Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset/2011_09_26/2011_09_26_drive_0048_sync/tracklet_labels.xml\n",
      "File contains 8 tracklets\n",
      "Loaded 8 tracklets.\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n",
      "[<source.parseTrackletXML.Tracklet object at 0x10ecf5438>, <source.parseTrackletXML.Tracklet object at 0x10ecf5470>, <source.parseTrackletXML.Tracklet object at 0x10ecf54a8>, <source.parseTrackletXML.Tracklet object at 0x10ecf54e0>, <source.parseTrackletXML.Tracklet object at 0x10ecf5518>, <source.parseTrackletXML.Tracklet object at 0x10ecf5550>, <source.parseTrackletXML.Tracklet object at 0x10ecf5588>, <source.parseTrackletXML.Tracklet object at 0x10ecf55c0>]\n"
     ]
    }
   ],
   "source": [
    "date = '2011_09_26'\n",
    "drive = '0048'\n",
    "dataset = load_dataset(date, drive,calibrated=True)\n",
    "\n",
    "\n",
    "directory = \"/Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset\"\n",
    "tracklet_rects, tracklet_types = load_tracklets_for_frames(len(list(dataset.velo)), '{}/{}/{}_drive_{}_sync/tracklet_labels.xml'.format(directory,date, date, drive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.001  -1.      0.0043 -0.7846]\n",
      " [ 0.0084 -0.0043 -1.      0.7195]\n",
      " [ 1.      0.001   0.0084 -1.0891]\n",
      " [ 0.      0.      0.      1.    ]]\n",
      "[[ 0.0002 -0.9999 -0.0106 -0.4734]\n",
      " [ 0.0104  0.0106 -0.9999 -0.0751]\n",
      " [ 0.9999  0.0001  0.0105 -0.2721]\n",
      " [ 0.      0.      0.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset.calib.T_cam3_imu)\n",
    "print(dataset.calib.T_cam3_velo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_velo_points(pts3d_raw):\n",
    "    '''Replaces the reflectance value by 1, and tranposes the array, so\n",
    "       points can be directly multiplied by the camera projection matrix'''\n",
    "\n",
    "    pts3d = pts3d_raw\n",
    "    # Reflectance > 0\n",
    "    pts3d = pts3d[pts3d[:, 3] > 0 ,:]\n",
    "    pts3d[:,3] = 1\n",
    "    return pts3d.transpose()\n",
    "\n",
    "def project_velo_points_in_img(pts3d, T_cam_velo, Rrect, Prect):\n",
    "    '''Project 3D points into 2D image. Expects pts3d as a 4xN\n",
    "       numpy array. Returns the 2D projection of the points that\n",
    "       are in front of the camera only an the corresponding 3D points.'''\n",
    "\n",
    "    # 3D points in camera reference frame.\n",
    "    pts3d_cam = Rrect.dot(T_cam_velo.dot(pts3d))\n",
    "\n",
    "    # Before projecting, keep only points with z>0 \n",
    "    # (points that are in fronto of the camera).\n",
    "    idx = (pts3d_cam[2,:]>=0)\n",
    "    pts2d_cam = Prect.dot(pts3d_cam[:,idx])\n",
    "\n",
    "    return pts3d[:, idx], pts2d_cam/pts2d_cam[2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-12-a7ae2a37ec9e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-a7ae2a37ec9e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print pykitti.__\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "print pykitti.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 0\n",
    "dataset_velo = list(dataset.velo)\n",
    "points = 0.8 # this controls the sampling rate\n",
    "points_step = int(1. / points)\n",
    "point_size = 0.01 * (1. / points)\n",
    "\n",
    "velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "velo_frame = dataset_velo[frame][velo_range, :]  \n",
    "\n",
    "\n",
    "\n",
    "print(dataset_velo[0].shape)\n",
    "print(velo_frame.shape)\n",
    "print(velo_frame.size)\n",
    "velo_points = prepare_velo_points(velo_frame)\n",
    "print(velo_points.shape)\n",
    "\n",
    "pts3d, pts2dcam = project_velo_points_in_img(velo_points,dataset.calib.T_velo_imu,dataset.calib.R_rect_30,dataset.calib.P_rect_30)\n",
    "\n",
    "print(pts2dcam.shape)\n",
    "print(dataset_rgb[frame][1].shape)\n",
    "# Create figure and axes\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(pts2dcam)\n",
    "\n",
    "#ax.imshow(dataset_rgb[frame][1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(pts2dcam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from source import dataset_utility as du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def parse_string_variable(str):\n",
    "    var_name = str.split(':')[0]\n",
    "    after_colon_index = len(var_name) + 1\n",
    "    value = str[after_colon_index:]\n",
    "    return (var_name, value)\n",
    "\n",
    "def read_lines_to_dict(raw_text):\n",
    "    var_list = []\n",
    "    for i, line in enumerate(raw_text):\n",
    "        var_list.append(line.replace('\\n', ''))\n",
    "    for i, line in enumerate(raw_text):\n",
    "        var_list[i] = parse_string_variable(line)\n",
    "    return dict(var_list)\n",
    "\n",
    "def read_files_by_lines(filename):\n",
    "    assert type(filename) is str\n",
    "    with open(filename, 'r') as cam_to_cam:\n",
    "#         data = cam_to_cam.read().replace('\\n', 'r')\n",
    "        data = cam_to_cam.readlines()\n",
    "    return read_lines_to_dict(data)\n",
    "\n",
    "def replace_var_from_dict_with_shape(var_dict, key, shape):\n",
    "    return np.array(var_dict[key]).reshape(shape)\n",
    "\n",
    "\n",
    "# TODO: \n",
    "# 1 if calibration completely found?\n",
    "# 2 if rectification available\n",
    "# 3 Deal with delta_f delta_t\n",
    "\n",
    "def loadCalibrationCamToCam(filename, verbose=False):\n",
    "    assert type(filename) is str\n",
    "    cam_dict = read_files_by_lines(filename)\n",
    "\n",
    "    for key, value in cam_dict.items():\n",
    "        if key == 'calib_time':\n",
    "            cam_dict[key] = value\n",
    "        else:\n",
    "            array = []\n",
    "            for i, string in enumerate(value.split(' ')[1:]):\n",
    "                array.append(float(string))\n",
    "            cam_dict[key] = array\n",
    "\n",
    "    for i in range(0, 4):\n",
    "        S_rect_0i = 'S_rect_0' + str(i)\n",
    "        R_rect_0i = 'R_rect_0' + str(i)\n",
    "        P_rect_0i = 'P_rect_0' + str(i)\n",
    "        S_0i = 'S_0' + str(i)\n",
    "        K_0i = 'K_0' + str(i)\n",
    "        D_0i = 'D_0' + str(i)\n",
    "        R_0i = 'R_0' + str(i)\n",
    "        T_0i = 'T_0' + str(i)\n",
    "\n",
    "        cam_dict[S_rect_0i] = replace_var_from_dict_with_shape(cam_dict, S_rect_0i, (1, 2))\n",
    "        cam_dict[R_rect_0i] = replace_var_from_dict_with_shape(cam_dict, R_rect_0i, (3, 3))\n",
    "        cam_dict[P_rect_0i] = replace_var_from_dict_with_shape(cam_dict, P_rect_0i, (3, 4))\n",
    "        cam_dict[S_0i] = replace_var_from_dict_with_shape(cam_dict, S_0i, (1, 2))\n",
    "        cam_dict[K_0i] = replace_var_from_dict_with_shape(cam_dict, K_0i, (3, 3))\n",
    "        cam_dict[D_0i] = replace_var_from_dict_with_shape(cam_dict, D_0i, (1, 5))\n",
    "        cam_dict[R_0i] = replace_var_from_dict_with_shape(cam_dict, R_0i, (3, 3))\n",
    "        cam_dict[T_0i] = replace_var_from_dict_with_shape(cam_dict, T_0i, (3, 1))\n",
    "\n",
    "    if verbose:\n",
    "          print(S_rect_0i, cam_dict[S_rect_0i])\n",
    "          print(R_rect_0i, cam_dict[R_rect_0i])\n",
    "          print(P_rect_0i, cam_dict[P_rect_0i])\n",
    "          print(S_0i, cam_dict[S_0i])\n",
    "          print(K_0i, cam_dict[K_0i])\n",
    "          print(D_0i, cam_dict[D_0i])\n",
    "          print(R_0i, cam_dict[R_0i])\n",
    "          print(T_0i, cam_dict[T_0i])\n",
    "    return cam_dict\n",
    "\n",
    "def loadCalibrationRigid(filename, verbose=False):\n",
    "    assert type(filename) is str\n",
    "    velo_dict = read_files_by_lines(filename)\n",
    "\n",
    "    for key, value in velo_dict.items():\n",
    "        if key == 'calib_time':\n",
    "            velo_dict[key] = value\n",
    "        else:\n",
    "            array = []\n",
    "            for i, string in enumerate(value.split(' ')[1:]):\n",
    "                array.append(float(string))\n",
    "            velo_dict[key] = array\n",
    "\n",
    "    R = 'R'\n",
    "    T = 'T'\n",
    "    velo_dict[R] = replace_var_from_dict_with_shape(velo_dict, R, (3, 3))\n",
    "    velo_dict[T] = replace_var_from_dict_with_shape(velo_dict, T, (3, 1))\n",
    "    # Tr = [R, T; 0 0 0 1]\n",
    "    Tr = np.vstack((np.hstack((velo_dict[R], velo_dict[T])), [0, 0, 0, 1]))\n",
    "    velo_dict['Tr'] = Tr\n",
    "\n",
    "    if verbose:\n",
    "      print(R, velo_dict[R])\n",
    "      print(T, velo_dict[T])\n",
    "      print('Tr', velo_dict['Tr'])\n",
    "    return velo_dict['Tr']\n",
    "\n",
    "# TODO: Limit to 2D matrix\n",
    "def project(p_in, T):\n",
    "#   Dimension of data projection matrix\n",
    "#    assert type(T) == 'numpy.ndarray'\n",
    "#    assert type(p_in) == 'numpy.ndarray'\n",
    "    dim_norm, dim_proj = T.shape\n",
    "\n",
    "    p_in_row_count = p_in.shape[0]\n",
    "#   Do transformation in homogenouous coordinates\n",
    "    p2_in = p_in\n",
    "    if p2_in.shape[1] < dim_proj:\n",
    "        col_ones = np.ones(p_in_row_count)\n",
    "        col_ones.shape = (p_in_row_count, 1)\n",
    "# matlab:       p2_in[:, dim_proj - 1] = 1\n",
    "        p2_in = np.hstack((p2_in, col_ones))\n",
    "#   (T*p2_in')'\n",
    "    p2_out = np.transpose(np.dot(T, np.transpose(p2_in)))\n",
    "#   Normalize homogeneous coordinates\n",
    "    denominator = np.outer(p2_out[:, dim_norm - 1], np.ones(dim_norm - 1))\n",
    "#   Element wise division\n",
    "    p_out = p2_out[:, 0: dim_norm-1]/denominator\n",
    "    return p_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "l_and = lambda *x: np.logical_and.reduce(x)\n",
    "def convert_velo_cord_to_img(data_set, calib_dir, cam=2, frame=20):\n",
    "    \"\"\"\n",
    "    Demostrates projection of the velodyne points into the image plane\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_dir  : Absolute path to sequence base directory (ends with _sync)\n",
    "    calib_dir : Absolute path to directory that contains calibration files\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "#     0-based index\n",
    "#     cam = 2\n",
    "#     frame = 20\n",
    "#     load calibration\n",
    "# TODO: use os.path.join?\n",
    "    calib = loadCalibrationCamToCam(calib_dir + 'calib_cam_to_cam.txt')\n",
    "    Tr_velo_to_cam = loadCalibrationRigid(calib_dir + 'calib_velo_to_cam.txt')\n",
    "\n",
    "#     Compute projection matrix velodyne->image plane\n",
    "    R_cam_to_rect = np.eye(4, dtype=float)\n",
    "    R_cam_to_rect[0: 3, 0: 3] = calib['R_rect_00']\n",
    "    P_velo_to_img = np.dot(np.dot(calib['P_rect_0' + str(cam)], R_cam_to_rect), Tr_velo_to_cam)\n",
    "#     print(type(R_cam_to_rect))\n",
    "#     Load image and display\n",
    "#   Load velodyne points\n",
    "# Take 1 of 5 points for display speed\n",
    "    #velo = data_set.velo[0:len(data_set.velo):5][0]\n",
    "    #velo = data_set.velo[0]\n",
    "#     print('data_set velo', data_set.velo[frame])\n",
    "    velo_data = dataset_velo[frame]\n",
    "    velo = velo_data[0:velo_data.shape[0]:5]\n",
    "    \n",
    "    #img_h, img_w, img_ch = dataset_rgb[frame].right.shape\n",
    "    img_h, img_w, img_ch = 400,1500,3\n",
    "    \n",
    "    img_plane_depth = 5\n",
    "    x_dir_pts = velo[:, 0]\n",
    "    filtered_x_dir_indices = l_and((x_dir_pts > img_plane_depth))\n",
    "#     .flatten to remove extra dimension\n",
    "    indices = np.argwhere(filtered_x_dir_indices).flatten()\n",
    "#     Depth (x) limited velodyne points\n",
    "    velo = velo[indices, :]\n",
    "#     Project to image plane (exclude luminance/intensity)\n",
    "    velo_img = project(velo[:, 0:3], P_velo_to_img)\n",
    "    \n",
    "    return velo_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_velo_to_img_size(img_shape, velo_data):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ----------\n",
    "    img_size: camera image size\n",
    "    velo_data :calibrated and project transformed lidar to camera data\n",
    "    \"\"\"\n",
    "    img_h = img_shape[0]\n",
    "    img_w = img_shape[1]\n",
    "    img_dim_x_pts = velo_data[:, 0]\n",
    "    img_dim_y_pts = velo_data[:, 1]\n",
    "    \n",
    "    x_filt = l_and((img_dim_x_pts < img_w), (img_dim_x_pts >= 0))\n",
    "    y_filt = l_and((img_dim_y_pts < img_h), (img_dim_y_pts >= 0))\n",
    "    filtered = l_and(x_filt, y_filt)\n",
    "    indices = np.argwhere(filtered).flatten()\n",
    "    \n",
    "    img_dim_x_pts = img_dim_x_pts[indices]\n",
    "    img_dim_y_pts = img_dim_y_pts[indices]\n",
    "    return (img_dim_x_pts, img_dim_y_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_velo[frame][velo_range, :] \n",
    "#dataset_velo[0:len(dataset_velo):5][0]\n",
    "#dataset.velo(1)\n",
    "#dataset.velo[0]\n",
    "#dataset_velo\n",
    "#type(dataset_rgb)\n",
    "\n",
    "\n",
    "#tmp_dataset_rgb = list(dataset.rgb)\n",
    "\n",
    "#type(tmp_dataset_rgb[frame][0].right)\n",
    "dataset.rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 0\n",
    "calib_dir =\"/Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset/2011_09_26/\"\n",
    "\n",
    "#velo_data = convert_velo_cord_to_img(dataset, calib_dir)\n",
    "velo_data = convert_velo_cord_to_img(dataset, calib_dir)\n",
    "#rgb_img = dataset.rgb[frame].right\n",
    "\n",
    "rgb_img =dataset_rgb[frame][1]\n",
    "#rgb_img = dataset.rgb[frame][0]\n",
    "#rgb_img = dataset\n",
    "#corped_velo_data = crop_velo_to_img_size(rgb_img.shape, velo_data)\n",
    "corped_velo_data = crop_velo_to_img_size([400,1500,3], velo_data)\n",
    "\n",
    "\n",
    "import cv2\n",
    "print(cv2.addWeighted)\n",
    "def overlay_velo_img(img, velo_data):\n",
    "    (x, y) = velo_data\n",
    "    im = np.zeros(img.shape, dtype=np.float32)\n",
    "    x_axis = np.floor(x).astype(np.int32)\n",
    "    y_axis = np.floor(y).astype(np.int32)\n",
    "#     im[y_axis, x_axis] = [1, 0, 1]\n",
    "    print(len(x))\n",
    "    print(len(y))\n",
    "    for i in range(0, len(x)):\n",
    "#         cv2.circle(img, center, radius, color, thickness=1, lineType=8, shift=0)\n",
    "        cv2.circle(img, (x_axis[i], y_axis[i]), 2, [0, 35, 0])\n",
    "    \n",
    "    fig1 = plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(img)\n",
    "\n",
    "overlay_velo_img(rgb_img, corped_velo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the points distributions, we could catch something meaningful if we limit **X**, **Y** and **Z** axis to some magic numbers.\n",
    "\n",
    "Additionally we will only visualise 20% of the point cloud, as each frame contains ~120K points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "colors = {\n",
    "    'Car': 'b',\n",
    "    'Tram': 'r',\n",
    "    'Cyclist': 'g',\n",
    "    'Van': 'c',\n",
    "    'Truck': 'm',\n",
    "    'Pedestrian': 'y',\n",
    "    'Sitter': 'k'\n",
    "}\n",
    "axes_limits = [\n",
    "    [-20, 80], # X axis range\n",
    "    [-20, 20], # Y axis range\n",
    "    [-3, 10]   # Z axis range\n",
    "]\n",
    "axes_str = ['X', 'Y', 'Z']\n",
    "\n",
    "def draw_box(pyplot_axis, vertices, axes=[0, 1, 2], color='black'):\n",
    "    \"\"\"\n",
    "    Draws a bounding 3D box in a pyplot axis.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pyplot_axis : Pyplot axis to draw in.\n",
    "    vertices    : Array 8 box vertices containing x, y, z coordinates.\n",
    "    axes        : Axes to use. Defaults to `[0, 1, 2]`, e.g. x, y and z axes.\n",
    "    color       : Drawing color. Defaults to `black`.\n",
    "    \"\"\"\n",
    "    vertices = vertices[axes, :]\n",
    "    connections = [\n",
    "        [0, 1], [1, 2], [2, 3], [3, 0],  # Lower plane parallel to Z=0 plane\n",
    "        [4, 5], [5, 6], [6, 7], [7, 4],  # Upper plane parallel to Z=0 plane\n",
    "        [0, 4], [1, 5], [2, 6], [3, 7]  # Connections between upper and lower planes\n",
    "    ]\n",
    "    for connection in connections:\n",
    "        pyplot_axis.plot(*vertices[:, connection], c=color, lw=0.5)\n",
    "\n",
    "def display_frame_statistics(dataset, tracklet_rects, tracklet_types, frame, points=0.2):\n",
    "    \"\"\"\n",
    "    Displays statistics for a single frame. Draws camera data, 3D plot of the lidar point cloud data and point cloud\n",
    "    projections to various planes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset         : `raw` dataset.\n",
    "    tracklet_rects  : Dictionary with tracklet bounding boxes coordinates.\n",
    "    tracklet_types  : Dictionary with tracklet types.\n",
    "    frame           : Absolute number of the frame.\n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "    \"\"\"\n",
    "    dataset_gray = list(dataset.gray)\n",
    "    dataset_rgb = list(dataset.rgb)\n",
    "    dataset_velo = list(dataset.velo)\n",
    "    \n",
    "    print('Frame timestamp: ' + str(dataset.timestamps[frame]))\n",
    "    # Draw camera data\n",
    "    f, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "    ax[0, 0].imshow(dataset_gray[frame][0], cmap='gray')\n",
    "    ax[0, 0].set_title('Left Gray Image (cam0)')\n",
    "    ax[0, 1].imshow(dataset_gray[frame][1], cmap='gray')\n",
    "    ax[0, 1].set_title('Right Gray Image (cam1)')\n",
    "    ax[1, 0].imshow(dataset_rgb[frame][0])\n",
    "    ax[1, 0].set_title('Left RGB Image (cam2)')\n",
    "    ax[1, 1].imshow(dataset_rgb[frame][1])\n",
    "    ax[1, 1].set_title('Right RGB Image (cam3)')\n",
    "    plt.show()\n",
    "\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :]      \n",
    "    def draw_point_cloud(ax, title, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n",
    "        \"\"\"\n",
    "        Convenient method for drawing various point cloud projections as a part of frame statistics.\n",
    "        \"\"\"\n",
    "        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='gray')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n",
    "        if len(axes) > 2:\n",
    "            ax.set_xlim3d(*axes_limits[axes[0]])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlim3d(*axes_limits[axes[2]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n",
    "        else:\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])\n",
    "        # User specified limits\n",
    "        if xlim3d!=None:\n",
    "            ax.set_xlim3d(xlim3d)\n",
    "        if ylim3d!=None:\n",
    "            ax.set_ylim3d(ylim3d)\n",
    "        if zlim3d!=None:\n",
    "            ax.set_zlim3d(zlim3d)\n",
    "            \n",
    "        for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "            draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n",
    "            \n",
    "    # Draw point cloud data as 3D plot\n",
    "    f2 = plt.figure(figsize=(15, 8))\n",
    "    ax2 = f2.add_subplot(111, projection='3d')                    \n",
    "    draw_point_cloud(ax2, 'Velodyne scan', xlim3d=(-10,30))\n",
    "    plt.show()\n",
    "    \n",
    "    # Draw point cloud data as plane projections\n",
    "    f, ax3 = plt.subplots(3, 1, figsize=(15, 25))\n",
    "    draw_point_cloud(\n",
    "        ax3[0], \n",
    "        'Velodyne scan, XZ projection (Y = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 2] # X and Z axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[1], \n",
    "        'Velodyne scan, XY projection (Z = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 1] # X and Y axes\n",
    "    )\n",
    "    draw_point_cloud(\n",
    "        ax3[2], \n",
    "        'Velodyne scan, YZ projection (X = 0), the car is moving towards the graph plane', \n",
    "        axes=[1, 2] # Y and Z axes\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    #print(dataset.oxts)\n",
    "    #print(velo_frame)\n",
    "#    print(*np.transpose(velo_frame[:, [0, 1, 2]]), s=point_size, c=velo_frame[:, 3], cmap='gray')\n",
    "#print(type(ax)) # this is a numpy array\n",
    "#print(velo_frame[:, 3])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0 probably y\n",
    "# 1 probably x\n",
    "# 2 probably z\n",
    "\n",
    "#velo_frame_test = velo_frame\n",
    "#velo_frame_test[:, 3]=velo_frame[:, 3]*10\n",
    "#plt.plot(velo_frame_test[:, 0],velo_frame_test[:, 3])\n",
    "#plt.show()\n",
    "\n",
    "# this seems correlated with the colors. the nearer the distance the higher the color. the further away the shorter the distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    frame = 10\n",
    "    dataset_velo = list(dataset.velo)\n",
    "    points = 0.8 # this controls the sampling rate\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    \n",
    "    velo_range = range(0, dataset_velo[10].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :] \n",
    "    velo_frame_test = velo_frame\n",
    "    velo_frame_test[:, 3]=velo_frame[:, 3]*10\n",
    "    velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :]  \n",
    "    velo_frame = velo_frame_test\n",
    "    def draw_point_cloud(ax, title, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n",
    "        \"\"\"\n",
    "        Convenient method for drawing various point cloud projections as a part of frame statistics.\n",
    "        \"\"\"\n",
    "        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='terrain')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n",
    "        #ax.patch.set_facecolor('black')\n",
    "        if len(axes) > 2:\n",
    "            ax.set_xlim3d(*axes_limits[axes[0]])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlim3d(*axes_limits[axes[2]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n",
    "        else:\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])\n",
    "        # User specified limits\n",
    "        if xlim3d!=None:\n",
    "            ax.set_xlim3d(xlim3d)\n",
    "        if ylim3d!=None:\n",
    "            ax.set_ylim3d(ylim3d)\n",
    "        if zlim3d!=None:\n",
    "            ax.set_zlim3d(zlim3d)\n",
    "            \n",
    "        for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "            draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n",
    "            \n",
    "    # Draw point cloud data as 3D plot\n",
    "    f2 = plt.figure(figsize=(15, 8))\n",
    "    ax2 = f2.add_subplot(111, projection='3d')                    \n",
    "    draw_point_cloud(ax2, 'Velodyne scan', xlim3d=(-10,30))\n",
    "    plt.show()\n",
    "    \n",
    "    f, ax3 = plt.subplots(1, 1, figsize=(15, 12))\n",
    "    draw_point_cloud(\n",
    "        ax3, \n",
    "        'Velodyne scan, XY projection (Z = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 1] # X and Y axes\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    frame = 10\n",
    "    dataset_velo = list(dataset.velo)\n",
    "    points = 0.8 # this controls the sampling rate\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    \n",
    "    velo_frame_test = velo_frame\n",
    "    velo_frame_test[:, 3]=velo_frame[:, 3]*10\n",
    "    velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :]  \n",
    "    velo_frame = velo_frame_test\n",
    "    def draw_point_cloud(ax, title, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n",
    "        \"\"\"\n",
    "        Convenient method for drawing various point cloud projections as a part of frame statistics.\n",
    "        \"\"\"\n",
    "        ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='terrain')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n",
    "        ax.set_facecolor(\"black\")\n",
    "        ax.grid(b='off')\n",
    "        \n",
    "        #ax.axis(b='off')\n",
    "        #fig.patch.set_visible(False)\n",
    "        #ax.patch.set_facecolor('black')\n",
    "        if len(axes) > 2:\n",
    "            ax.set_xlim3d(*axes_limits[axes[0]])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlim3d(*axes_limits[axes[2]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n",
    "            \n",
    "            # make pane colors transparents\n",
    "            # these are called axes pane colors. \n",
    "            #https://stackoverflow.com/questions/44001613/matplotlip-3d-surface-plot-turn-off-background-but-keep-axes\n",
    "            ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "            ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "            ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "        else:\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])\n",
    "        # User specified limits\n",
    "        if xlim3d!=None:\n",
    "            ax.set_xlim3d(xlim3d)\n",
    "        if ylim3d!=None:\n",
    "            ax.set_ylim3d(ylim3d)\n",
    "        if zlim3d!=None:\n",
    "            ax.set_zlim3d(zlim3d)\n",
    "            \n",
    "        for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "            draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n",
    "            \n",
    "    # Draw point cloud data as 3D plot\n",
    "    f2 = plt.figure(figsize=(15, 8))\n",
    "    #f2.patch.set_visible(False)\n",
    "    ax2 = f2.add_subplot(111, projection='3d')                    \n",
    "    draw_point_cloud(ax2, 'Velodyne scan', xlim3d=(-10,30))\n",
    "    plt.show()\n",
    "    \n",
    "    f, ax3 = plt.subplots(1, 1, figsize=(15, 12))\n",
    "    draw_point_cloud(\n",
    "        ax3, \n",
    "        'Velodyne scan, XY projection (Z = 0), the car is moving in direction left to right', \n",
    "        axes=[0, 1] # X and Y axes\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step #1: Get one frame of image and lidar points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "\n",
    "display_frame_statistics(dataset, tracklet_rects, tracklet_types, frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    f, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "    ax[0, 0].imshow(dataset_gray[frame][0], cmap='gray')\n",
    "    ax[0, 0].set_title('Left Gray Image (cam0)')\n",
    "    ax[0, 1].imshow(dataset_gray[frame][1], cmap='gray')\n",
    "    ax[0, 1].set_title('Right Gray Image (cam1)')\n",
    "    ax[1, 0].imshow(dataset_rgb[frame][0])\n",
    "    ax[1, 0].set_title('Left RGB Image (cam2)')\n",
    "    ax[1, 1].imshow(dataset_rgb[frame][1])\n",
    "    ax[1, 1].set_title('Right RGB Image (cam3)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Annotate camera image\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Create figure and axes\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(dataset_rgb[frame][1])\n",
    "\n",
    "# Create a Rectangle patch\n",
    "rect = patches.Rectangle((280,150),200,200,linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "# Add the patch to the Axes\n",
    "ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "#    draw_box(ax, t_rects, axes=axes, color=colors[t_type])\n",
    "print(tracklet_rects[0])\n",
    "print(tracklet_types[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utilities import print_progress\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "directory = '/home/ubuntu/AI/KITTI-Dataset/'\n",
    "input_axes=[[0, 1, 2],[1, 2],[0, 1]]\n",
    "#input_axes=[1, 2]\n",
    "\n",
    "def draw_3d_plot(frame, dataset, tracklet_rects, tracklet_types,axes, points=0.2):\n",
    "    \"\"\"\n",
    "    Saves a single frame for an animation: a 3D plot of the lidar data without ticks and all frame trackelts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame           : Absolute number of the frame.\n",
    "    dataset         : `raw` dataset.\n",
    "    tracklet_rects  : Dictionary with tracklet bounding boxes coordinates.\n",
    "    tracklet_types  : Dictionary with tracklet types.\n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Saved frame filename.\n",
    "    \"\"\"\n",
    "    dataset_velo = list(dataset.velo)\n",
    "    \n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    axis = f.add_subplot(111, projection='3d', xticks=[], yticks=[], zticks=[])\n",
    "\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :]\n",
    "    axis.scatter(*np.transpose(velo_frame[:, [0, 1, 2]]), s=point_size, c=velo_frame[:, 3], cmap='terrain')\n",
    "    axis.set_xlim3d(*axes_limits[0])\n",
    "    axis.set_ylim3d(*axes_limits[1])\n",
    "    axis.set_zlim3d(*axes_limits[2])\n",
    "    \n",
    "    axis.set_facecolor(\"black\")\n",
    "    axis.grid(b='off')\n",
    "\n",
    "    axis.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    axis.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    axis.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    \n",
    "    # i think this maybe overlaying it some how. but still making progress on this. at least we know what it does.\n",
    "    # actually we could reduce the script to this function\n",
    "    axis.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='terrain')\n",
    "    \n",
    "    # this is for the bounding box\n",
    "    for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "        #draw_box(axis, t_rects, axes=[0, 1, 2], color=colors[t_type])\n",
    "        draw_box(axis, t_rects, axes, color=colors[t_type])\n",
    "    filename = directory + 'video/frame_{0:0>4}.png'.format(frame)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(f)\n",
    "    return filename\n",
    "\n",
    "frames = []\n",
    "n_frames = len(list(dataset.velo))\n",
    "\n",
    "n_frames = 10\n",
    "\n",
    "for a in input_axes:\n",
    "    print('Preparing animation frames...')\n",
    "    frames = []\n",
    "    for i in range(n_frames):\n",
    "        print_progress(i, n_frames - 1)\n",
    "        filename = draw_3d_plot(i, dataset, tracklet_rects, tracklet_types,a)\n",
    "        frames += [filename]\n",
    "    print('...Animation frames ready.')\n",
    "\n",
    "    clip = ImageSequenceClip(frames, fps=5)\n",
    "    % time\n",
    "    clip.write_gif('pcl_data{0}.gif'.format(a), fps=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2. How to make a movie to explorate this\n",
    "\n",
    "import cv2\n",
    "\n",
    "img1 = cv2.imread('1.jpg')\n",
    "img2 = cv2.imread('2.jpg')\n",
    "img3 = cv2.imread('3.jpg')\n",
    "\n",
    "height , width , layers =  img1.shape\n",
    "\n",
    "video = cv2.VideoWriter('video.avi',-1,1,(width,height))\n",
    "\n",
    "video.write(img1)\n",
    "video.write(img2)\n",
    "video.write(img3)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_axes=[[0, 1, 2],[1, 2],[0, 1]]\n",
    "print(input_axes)\n",
    "print(input_axes[1])\n",
    "\n",
    "range(input_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#backup\n",
    "\n",
    "from source.utilities import print_progress\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "directory = '/Users/davidwoo/Documents/Projects/self-driving-cars/plane-segmentation/KITTI-Dataset'\n",
    "input_axes=[[0, 1, 2],[1, 2],[0, 1]]\n",
    "#input_axes=[1, 2]\n",
    "\n",
    "def draw_3d_plot(frame, dataset, tracklet_rects, tracklet_types,axes, points=0.2):\n",
    "    \"\"\"\n",
    "    Saves a single frame for an animation: a 3D plot of the lidar data without ticks and all frame trackelts.\n",
    "    Parameters\n",
    "    ----------\n",
    "    frame           : Absolute number of the frame.\n",
    "    dataset         : `raw` dataset.\n",
    "    tracklet_rects  : Dictionary with tracklet bounding boxes coordinates.\n",
    "    tracklet_types  : Dictionary with tracklet types.\n",
    "    points          : Fraction of lidar points to use. Defaults to `0.2`, e.g. 20%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Saved frame filename.\n",
    "    \"\"\"\n",
    "    dataset_velo = list(dataset.velo)\n",
    "    \n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    axis = f.add_subplot(111, projection='3d', xticks=[], yticks=[], zticks=[])\n",
    "\n",
    "    points_step = int(1. / points)\n",
    "    point_size = 0.01 * (1. / points)\n",
    "    velo_range = range(0, dataset_velo[frame].shape[0], points_step)\n",
    "    velo_frame = dataset_velo[frame][velo_range, :]\n",
    "    axis.scatter(*np.transpose(velo_frame[:, [0, 1, 2]]), s=point_size, c=velo_frame[:, 3], cmap='terrain')\n",
    "    axis.set_xlim3d(*axes_limits[0])\n",
    "    axis.set_ylim3d(*axes_limits[1])\n",
    "    axis.set_zlim3d(*axes_limits[2])\n",
    "    \n",
    "    axis.set_facecolor(\"black\")\n",
    "    axis.grid(b='off')\n",
    "\n",
    "    axis.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    axis.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    axis.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "    \n",
    "    # this is for the bounding box\n",
    "    for t_rects, t_type in zip(tracklet_rects[frame], tracklet_types[frame]):\n",
    "        #draw_box(axis, t_rects, axes=[0, 1, 2], color=colors[t_type])\n",
    "        draw_box(axis, t_rects, axes, color=colors[t_type])\n",
    "    filename = directory + 'video/frame_{0:0>4}.png'.format(frame)\n",
    "    plt.savefig(filename)\n",
    "    plt.close(f)\n",
    "    return filename\n",
    "\n",
    "frames = []\n",
    "n_frames = len(list(dataset.velo))\n",
    "\n",
    "n_frames = 20\n",
    "\n",
    "for a in input_axes:\n",
    "    print('Preparing animation frames...')\n",
    "    frames = []\n",
    "    for i in range(n_frames):\n",
    "        print_progress(i, n_frames - 1)\n",
    "        filename = draw_3d_plot(i, dataset, tracklet_rects, tracklet_types,a)\n",
    "        frames += [filename]\n",
    "    print('...Animation frames ready.')\n",
    "\n",
    "    clip = ImageSequenceClip(frames, fps=5)\n",
    "    % time\n",
    "    clip.write_gif('pcl_data{0}.gif'.format(a), fps=5)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
